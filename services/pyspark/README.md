# PySpark Service with VAST DATA Integration

This service provides a complete Apache Spark cluster with VAST DATA VastDB connector integration using the `vastdataorg/spark-vast` Docker image.

## Services

- **spark-master**: Spark Master node with Web UI on port 8090
- **spark-worker**: Spark Worker node with Web UI on port 8081  
- **spark-client**: Spark Client for submitting jobs with Driver Web UI on port 4040

## Configuration

The Spark configuration is managed through environment variables in the `.env` file. The VAST DATA cluster details are automatically loaded from these environment variables:

```
SPARK_NDB_ENDPOINT=http://YOUR-VAST-VIP
SPARK_NDB_DATAENDPOINTS=http://YOUR-VAST-VIP1,http://YOUR-VAST-VIP2
SPARK_NDB_ACCESS_KEY_ID=YOUR-ACCESS-KEY
SPARK_NDB_SECRET_ACCESS_KEY=YOUR-SECRET-KEY
```

These values are already configured in your `.env` file and will be automatically substituted into the Spark configuration when the containers start.

## Usage

### Starting the Spark Cluster

```bash
# Start all services
docker-compose up -d spark-master spark-worker spark-client

# Or start individual services
docker-compose up -d spark-master
docker-compose up -d spark-worker
docker-compose up -d spark-client
```

### Running Spark SQL

To run interactive Spark SQL queries against VastDB:

```bash
docker exec -it spark-client /opt/spark/bin/spark-sql --master spark://spark-master:7077 \
  --driver-class-path $(echo '/opt/spark/vast/*.jar' | tr ' ' ':') \
  --conf spark.executor.extraClassPath=$(echo '/opt/spark/vast/*.jar' | tr ' ' ':') \
  --jars $(echo '/opt/spark/vast/*.jar' | tr ' ' ',') \
  --conf spark.executor.userClassPathFirst=true \
  --conf spark.driver.userClassPathFirst=true \
  --conf spark.driver.maxResultSize=4g \
  --conf spark.driver.memory=16g \
  --conf spark.executor.cores=8 \
  --conf spark.executor.memory=8g \
  --conf spark.sql.catalogImplementation=in-memory
```

### Running PySpark

To run PySpark applications:

```bash
docker exec -it spark-client /opt/spark/bin/pyspark --master spark://spark-master:7077 \
  --driver-class-path $(echo '/opt/spark/vast/*.jar' | tr ' ' ':') \
  --conf spark.executor.extraClassPath=$(echo '/opt/spark/vast/*.jar' | tr ' ' ':') \
  --jars $(echo '/opt/spark/vast/*.jar' | tr ' ' ',')
```

### Example Queries

Once in Spark SQL prompt, you can query VastDB tables:

```sql
-- List available databases
SHOW DATABASES;

-- Query a specific table
SELECT * FROM `ndb`.`vastdb`.`schema1`.`table1` LIMIT 10;

-- Create a temporary view from VastDB data
CREATE OR REPLACE TEMPORARY VIEW temp_view AS
SELECT * FROM `ndb`.`vastdb`.`schema1`.`table1`;

-- Query the temporary view
SELECT COUNT(*) FROM temp_view;
```

## Web UIs

- **Spark Master UI**: http://localhost:8090
- **Spark Worker UI**: http://localhost:8081
- **Spark Driver UI**: http://localhost:4040 (when running jobs)

## Stopping Services

```bash
# Stop all Spark services
docker-compose stop spark-master spark-worker spark-client

# Remove containers
docker-compose rm -f spark-master spark-worker spark-client
```

## Troubleshooting

1. **Connection Issues**: Ensure your VAST DATA cluster is accessible and the VIP endpoints are correct
2. **Authentication**: Verify your access key and secret key are valid
3. **Memory Issues**: Adjust memory settings in spark-defaults.conf based on your system resources
4. **Network**: Ensure all containers are on the same network (zeek-network)

## Integration with Other Services

This PySpark cluster can be used to process data from:
- Zeek logs captured by the zeek-live service
- SIEM events generated by the siem-simulator
- Data flowing through the fluentd service

The services are all connected via the `zeek-network` Docker network for seamless communication.
